{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pXRwUtwU8Te5",
        "outputId": "eb0b99f7-ba8d-400d-90da-786bb2a2da11"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "7ufRoFuK_pyT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pip install pandas\n",
        "pip install sqlite3\n",
        "pip install numpy"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gq5o6cb0uynj",
        "outputId": "49ee9191-397b-49ed-91c1-8602c9e2fc99"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (2.2.2)\n",
            "Requirement already satisfied: numpy>=1.26.0 in /usr/local/lib/python3.12/dist-packages (from pandas) (2.0.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import sqlite3\n",
        "import numpy as np\n",
        "import os\n",
        "\n",
        "# --- 1. SETUP: SIMULATE GEE DATA DOWNLOAD ---\n",
        "# In a real project, you would replace this with:\n",
        "df_raw = pd.read_csv('Mangrove_Health_TimeSeries_FIXED.csv')\n",
        "\n",
        "\n",
        "# --- 2. DIMENSION TABLE GENERATION ---\n",
        "\n",
        "# 2.1 DIM_TIME\n",
        "def create_dim_time(df):\n",
        "    \"\"\"Generates the Dim_Time table from the unique acquisition dates.\"\"\"\n",
        "    df_time = df[['Date_Acquired']].drop_duplicates().copy()\n",
        "    df_time['Date_Acquired'] = pd.to_datetime(df_time['Date_Acquired'])\n",
        "\n",
        "    df_time['Year'] = df_time['Date_Acquired'].dt.year\n",
        "    df_time['Month'] = df_time['Date_Acquired'].dt.month\n",
        "    df_time['Day_of_Year'] = df_time['Date_Acquired'].dt.dayofyear\n",
        "\n",
        "    # Simple seasonal classification (adjust for local monsoon dates if needed)\n",
        "    def classify_season(month):\n",
        "        if 6 <= month <= 9: return 'Monsoon'\n",
        "        if month == 10 or month == 11: return 'Post-Monsoon'\n",
        "        if 1 <= month <= 5 or month == 12: return 'Dry'\n",
        "        return 'Other'\n",
        "\n",
        "    df_time['Season'] = df_time['Month'].apply(classify_season)\n",
        "    df_time['Date_ID'] = (df_time['Date_Acquired'].dt.strftime('%Y%m%d')).astype(int)\n",
        "\n",
        "    # Select final columns and set Date_ID as the Primary Key\n",
        "    df_time = df_time[['Date_ID', 'Date_Acquired', 'Year', 'Month', 'Day_of_Year', 'Season']]\n",
        "    return df_time.rename(columns={'Date_Acquired': 'Full_Date'})\n",
        "\n",
        "# 2.2 DIM_LOCATION\n",
        "def create_dim_location(df):\n",
        "    \"\"\"Generates the Dim_Location table from unique spatial attributes.\"\"\"\n",
        "    # Location is uniquely defined by Lat/Lon and Subregion\n",
        "    df_location = df[['AOI_Subregion', 'Longitude', 'Latitude']].drop_duplicates().copy()\n",
        "    df_location.reset_index(drop=True, inplace=True)\n",
        "    df_location['Location_ID'] = df_location.index + 1 # Simple integer PK\n",
        "\n",
        "    # Placeholder for Shoreline Proximity (would be derived via GIS buffer analysis)\n",
        "    df_location['Shoreline_Proximity'] = np.select(\n",
        "        [df_location['AOI_Subregion'] == 'Panvel',\n",
        "         df_location['AOI_Subregion'] == 'Uran'],\n",
        "        ['Interior', 'Fringe'],\n",
        "        default='Interior'\n",
        "    )\n",
        "\n",
        "    df_location = df_location[['Location_ID', 'Longitude', 'Latitude', 'AOI_Subregion', 'Shoreline_Proximity']]\n",
        "    return df_location\n",
        "\n",
        "# 2.3 DIM_SENSOR\n",
        "def create_dim_sensor():\n",
        "    \"\"\"Generates the Dim_Sensor table (Static).\"\"\"\n",
        "    data = {\n",
        "        'Sensor_ID': [1],\n",
        "        'Satellite_Name': ['Sentinel-2A'],\n",
        "        'Resolution_m': [10],\n",
        "        'Data_Source': ['GEE/COPERNICUS/S2_SR_HARMONIZED']\n",
        "    }\n",
        "    df_sensor = pd.DataFrame(data)\n",
        "    return df_sensor\n",
        "\n",
        "\n",
        "# --- 3. FACT TABLE GENERATION ---\n",
        "def create_fact_table(df_raw, df_time, df_location, df_sensor):\n",
        "    \"\"\"Merges Dimensions back into the Raw data to create the Fact Table.\"\"\"\n",
        "    df_fact = df_raw.copy()\n",
        "    df_fact.rename(columns={'Date_Acquired': 'Full_Date'}, inplace=True)\n",
        "\n",
        "    # Merge Dim_Time (to get Date_ID)\n",
        "    df_fact = pd.merge(df_fact, df_time[['Full_Date', 'Date_ID']], on='Full_Date', how='left')\n",
        "\n",
        "    # Merge Dim_Location (to get Location_ID)\n",
        "    df_fact = pd.merge(df_fact, df_location[['Location_ID', 'AOI_Subregion', 'Longitude', 'Latitude']],\n",
        "                       on=['AOI_Subregion', 'Longitude', 'Latitude'], how='left')\n",
        "\n",
        "    # Assign Sensor_ID (assuming all data comes from Sentinel-2A/2B mix, use Sensor_ID=1 as primary key)\n",
        "    # Note: In a real scenario, the GEE export would specify which satellite acquired the image.\n",
        "    df_fact['Sensor_ID'] = 1\n",
        "\n",
        "    # Create the final Fact Table structure\n",
        "    df_fact['Mangrove_Flag'] = (df_fact['NDVI_Value'] > 0.4).astype(int)\n",
        "\n",
        "    df_fact = df_fact[[\n",
        "        'Date_ID', 'Location_ID', 'Sensor_ID',\n",
        "        'NDVI_Value', 'EVI_Value', 'Mangrove_Flag'\n",
        "        # Other measures like NDVI_Change are calculated separately on aggregate data\n",
        "    ]]\n",
        "\n",
        "    df_fact.reset_index(drop=True, inplace=True)\n",
        "    df_fact['Fact_ID'] = df_fact.index + 1\n",
        "\n",
        "    return df_fact\n",
        "\n",
        "# --- 4. EXECUTION AND DATABASE CREATION ---\n",
        "def build_star_schema_db(db_name='mangrove_analysis.db'):\n",
        "    \"\"\"Main function to build the Star Schema and store in SQLite.\"\"\"\n",
        "\n",
        "    if os.path.exists(db_name):\n",
        "        os.remove(db_name)\n",
        "\n",
        "    # 1. Prepare DataFrames\n",
        "    df_raw = create_sample_gee_data()\n",
        "    df_time = create_dim_time(df_raw)\n",
        "    df_location = create_dim_location(df_raw)\n",
        "    df_sensor = create_dim_sensor()\n",
        "    df_fact = create_fact_table(df_raw, df_time, df_location, df_sensor)\n",
        "\n",
        "    # 2. Connect to SQLite Database\n",
        "    conn = sqlite3.connect(db_name)\n",
        "    cursor = conn.cursor()\n",
        "\n",
        "    # 3. Write DataFrames to Database (OLAP Load)\n",
        "    df_time.to_sql('Dim_Time', conn, if_exists='replace', index=False)\n",
        "    df_location.to_sql('Dim_Location', conn, if_exists='replace', index=False)\n",
        "    df_sensor.to_sql('Dim_Sensor', conn, if_exists='replace', index=False)\n",
        "    df_fact.to_sql('Fact_Mangrove_Health', conn, if_exists='replace', index=False)\n",
        "\n",
        "    # 4. Verification and Example Query (OLAP Query)\n",
        "    print(\"\\n--- Star Schema Built Successfully ---\")\n",
        "    print(f\"Database created: {db_name}\")\n",
        "    print(\"\\nExample OLAP Query (Mean NDVI by Subregion and Year):\")\n",
        "\n",
        "    query = \"\"\"\n",
        "    SELECT\n",
        "        T.Year,\n",
        "        L.AOI_Subregion,\n",
        "        AVG(F.NDVI_Value) AS Avg_NDVI\n",
        "    FROM\n",
        "        Fact_Mangrove_Health F\n",
        "    JOIN\n",
        "        Dim_Time T ON F.Date_ID = T.Date_ID\n",
        "    JOIN\n",
        "        Dim_Location L ON F.Location_ID = L.Location_ID\n",
        "    GROUP BY\n",
        "        T.Year, L.AOI_Subregion\n",
        "    ORDER BY\n",
        "        L.AOI_Subregion, T.Year;\n",
        "    \"\"\"\n",
        "\n",
        "    results = pd.read_sql_query(query, conn)\n",
        "    print(results.to_string())\n",
        "\n",
        "    conn.close()\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    build_star_schema_db()\n"
      ],
      "metadata": {
        "id": "2ihuDjyL_qTC",
        "outputId": "691ca05c-f8b9-440f-9d5f-978502a609eb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Creating sample GEE data...\n",
            "\n",
            "--- Star Schema Built Successfully ---\n",
            "Database created: mangrove_analysis.db\n",
            "\n",
            "Example OLAP Query (Mean NDVI by Subregion and Year):\n",
            "    Year AOI_Subregion  Avg_NDVI\n",
            "0   2018        Panvel  0.695700\n",
            "1   2019        Panvel  0.687250\n",
            "2   2020        Panvel  0.663750\n",
            "3   2021        Panvel  0.662033\n",
            "4   2022        Panvel  0.632567\n",
            "5   2023        Panvel  0.616967\n",
            "6   2024        Panvel  0.609500\n",
            "7   2018         Thane  0.699250\n",
            "8   2019         Thane  0.676850\n",
            "9   2020         Thane  0.659900\n",
            "10  2021         Thane  0.659667\n",
            "11  2022         Thane  0.636850\n",
            "12  2023         Thane  0.633750\n",
            "13  2024         Thane  0.616467\n",
            "14  2018          Uran  0.691033\n",
            "15  2019          Uran  0.675417\n",
            "16  2020          Uran  0.671250\n",
            "17  2021          Uran  0.646183\n",
            "18  2022          Uran  0.640417\n",
            "19  2023          Uran  0.629100\n",
            "20  2024          Uran  0.614583\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-4251160162.py:13: FutureWarning: 'M' is deprecated and will be removed in a future version, please use 'ME' instead.\n",
            "  dates = pd.date_range(start='2018-01-01', end='2024-12-31', freq='2M')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "9ZTfiIDrJ0HJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import os\n",
        "from datetime import datetime\n",
        "\n",
        "# --- CONFIGURATION ---\n",
        "DB_NAME = 'mangrove_analysis.db'\n",
        "CSV_IN_NAME = 'Mangrove_Health_TimeSeries_FIXED.csv' # Using the fixed GEE file name\n",
        "ARFF_OUT_NAME = 'mangrove_weka_analysis_FLAT.arff'\n",
        "\n",
        "# --- ARFF EXPORT FUNCTION (New) ---\n",
        "\n",
        "def create_arff_string(df, relation_name=\"mangrove_health_analysis\"):\n",
        "    \"\"\"Converts a Pandas DataFrame into a Weka-compatible ARFF string.\"\"\"\n",
        "    arff_content = []\n",
        "\n",
        "    # 1. Relation Declaration\n",
        "    arff_content.append(f\"@relation {relation_name}\\n\")\n",
        "\n",
        "    # 2. Attribute Declarations\n",
        "    for col in df.columns:\n",
        "        series = df[col]\n",
        "\n",
        "        if col in ['NDVI_Value', 'EVI_Value']:\n",
        "            # Explicitly set health indices as numeric\n",
        "            arff_content.append(f\"@attribute {col} numeric\")\n",
        "        elif series.dtype == 'object' or col in ['Year', 'Month', 'Season', 'AOI_Subregion', 'Shoreline_Proximity']:\n",
        "            # Nominal (Categorical) attributes\n",
        "            unique_values = series.dropna().unique()\n",
        "\n",
        "            # Sort year and month numerically, others alphabetically\n",
        "            if col in ['Year', 'Month']:\n",
        "                 unique_values = sorted([str(int(x)) for x in unique_values])\n",
        "            else:\n",
        "                 unique_values = sorted([str(x) for x in unique_values])\n",
        "\n",
        "            # Format nominal list: {val1, val2, val3,...}\n",
        "            nominal_list = '{' + ','.join(unique_values) + '}'\n",
        "            arff_content.append(f\"@attribute {col} {nominal_list}\")\n",
        "        else:\n",
        "            # All other columns (like internal IDs, if any remain)\n",
        "            arff_content.append(f\"@attribute {col} string\")\n",
        "\n",
        "    arff_content.append(\"\\n@data\")\n",
        "\n",
        "    # 3. Data Section\n",
        "    # Convert DataFrame to list of records (rows)\n",
        "    for index, row in df.iterrows():\n",
        "        row_values = []\n",
        "        for val in row:\n",
        "            if pd.isnull(val):\n",
        "                row_values.append('?') # Weka missing value syntax\n",
        "            else:\n",
        "                # Remove commas from nominal data to prevent Weka errors\n",
        "                row_values.append(str(val).replace(',', ''))\n",
        "        arff_content.append(','.join(row_values))\n",
        "\n",
        "    return \"\\n\".join(arff_content)\n",
        "\n",
        "\n",
        "# --- 1. DATA PREPARATION AND DIMENSION CREATION FUNCTIONS ---\n",
        "\n",
        "def create_dim_time(df):\n",
        "    \"\"\"Generates the Dim_Time table from the unique acquisition dates.\"\"\"\n",
        "    df_time = df[['Date_Acquired']].drop_duplicates().copy()\n",
        "    df_time['Date_Acquired'] = pd.to_datetime(df_time['Date_Acquired'])\n",
        "\n",
        "    df_time['Year'] = df_time['Date_Acquired'].dt.year.astype(str)\n",
        "    df_time['Month'] = df_time['Date_Acquired'].dt.month.astype(str)\n",
        "\n",
        "    def classify_season(month):\n",
        "        if 6 <= month <= 9: return 'Monsoon'\n",
        "        if month == 10 or month == 11: return 'Post_Monsoon'\n",
        "        return 'Dry'\n",
        "\n",
        "    # Ensure month is integer for comparison before converting to nominal string\n",
        "    df_time['Season'] = df_time['Month'].astype(int).apply(classify_season)\n",
        "\n",
        "    df_time = df_time[['Date_Acquired', 'Year', 'Month', 'Season']]\n",
        "    return df_time.rename(columns={'Date_Acquired': 'Full_Date'})\n",
        "\n",
        "def create_dim_location(df):\n",
        "    \"\"\"Generates the Dim_Location table, simulating the spatial breakdown (AOI Subregions).\"\"\"\n",
        "    # NOTE: AOI_Subregion is necessary for the OLAP Slice/Dice/Drill-down\n",
        "    # Assign subregions randomly for the time series data points.\n",
        "    subregions = ['Panvel', 'Uran', 'Thane_Creek']\n",
        "    df['AOI_Subregion'] = np.random.choice(subregions, size=len(df))\n",
        "\n",
        "    df_location = df[['AOI_Subregion']].drop_duplicates().copy()\n",
        "    df_location.reset_index(drop=True, inplace=True)\n",
        "\n",
        "    df_location['Shoreline_Proximity'] = np.select(\n",
        "        [df_location['AOI_Subregion'] == 'Uran',\n",
        "         df_location['AOI_Subregion'] == 'Thane_Creek'],\n",
        "        ['Fringe', 'Interior'],\n",
        "        default='Delta'\n",
        "    )\n",
        "    return df_location\n",
        "\n",
        "def export_weka_data():\n",
        "    \"\"\"Builds the Star Schema structure and exports a flat table as ARFF for Weka.\"\"\"\n",
        "    print(f\"Starting Weka Data Export Process. Reading: {CSV_IN_NAME}\")\n",
        "\n",
        "    if not os.path.exists(CSV_IN_NAME):\n",
        "        print(f\"ERROR: Cannot find {CSV_IN_NAME}. Please download the FIXED GEE CSV.\")\n",
        "        return\n",
        "\n",
        "    # Load the real GEE data\n",
        "    df_raw = pd.read_csv(CSV_IN_NAME)\n",
        "\n",
        "    # --- CRITICAL FIXES FOR CSV COLUMNS ---\n",
        "    # 1. Rename the primary GEE Date column to the internal standard 'Date_Acquired'\n",
        "    df_raw.rename(columns={'Date': 'Date_Acquired'}, inplace=True)\n",
        "\n",
        "    # 2. Clean up date formats and convert the time column\n",
        "    df_raw['Date_Acquired'] = pd.to_datetime(df_raw['Date_Acquired']).dt.normalize()\n",
        "\n",
        "    # 3. Rename the Index columns to standard names for clarity in ARFF\n",
        "    df_raw.rename(columns={'NDVI': 'NDVI_Value', 'EVI': 'EVI_Value'}, inplace=True)\n",
        "\n",
        "    # Check for empty file content\n",
        "    if df_raw.empty or df_raw['NDVI_Value'].isnull().all():\n",
        "        print(\"ERROR: CSV file is empty or contains no valid data. Please check the GEE export.\")\n",
        "        return\n",
        "\n",
        "    # --- Create Dimensions and Flatten ---\n",
        "    df_time = create_dim_time(df_raw)\n",
        "    df_location = create_dim_location(df_raw)\n",
        "\n",
        "    # Merge Time Dimension\n",
        "    df_weka = pd.merge(df_raw, df_time[['Full_Date', 'Year', 'Month', 'Season']],\n",
        "                       left_on='Date_Acquired', right_on='Full_Date', how='left')\n",
        "\n",
        "    # Merge Location Dimension (relies on AOI_Subregion simulation in create_dim_location)\n",
        "    df_weka = pd.merge(df_weka, df_location[['AOI_Subregion', 'Shoreline_Proximity']],\n",
        "                       on='AOI_Subregion', how='left')\n",
        "\n",
        "    # Select the final attributes Weka needs (dropping spectral bands, coordinates, and redundant time fields)\n",
        "    df_weka = df_weka[[\n",
        "        'Year', 'Month', 'Season', 'AOI_Subregion', 'Shoreline_Proximity',\n",
        "        'NDVI_Value', 'EVI_Value'\n",
        "    ]].dropna().copy()\n",
        "\n",
        "    # Final cleanup before ARFF export\n",
        "    df_weka['Year'] = df_weka['Year'].astype(str)\n",
        "    df_weka['Month'] = df_weka['Month'].astype(str)\n",
        "\n",
        "    # 4. Export to ARFF\n",
        "    arff_string = create_arff_string(df_weka)\n",
        "    with open(ARFF_OUT_NAME, 'w') as f:\n",
        "        f.write(arff_string)\n",
        "\n",
        "    print(\"\\n--- Python Export Successful ---\")\n",
        "    print(f\"Flattened Weka ARFF file saved as: {ARFF_OUT_NAME}\")\n",
        "    print(f\"Total records exported: {len(df_weka)}\")\n",
        "    print(\"\\nFile is ready for Weka Explorer (Clustering & Association Tabs).\")\n",
        "\n",
        "# Execute the process\n",
        "if __name__ == '__main__':\n",
        "    export_weka_data()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vcfPvFR-J0ZM",
        "outputId": "b60f8669-5fcb-4af5-d94f-178da02d9531"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting Weka Data Export Process. Reading: Mangrove_Health_TimeSeries_FIXED.csv\n",
            "\n",
            "--- Python Export Successful ---\n",
            "Flattened Weka ARFF file saved as: mangrove_weka_analysis_FLAT.arff\n",
            "Total records exported: 214\n",
            "\n",
            "File is ready for Weka Explorer (Clustering & Association Tabs).\n"
          ]
        }
      ]
    }
  ]
}